Barroso defende criação de legislação e órgão independente para monitorar redes sociais
Desde quando esteve na presidência do Tribunal Superior Eleitoral em 2020, o ministro do Supremo Tribunal Federal Luís Roberto Barroso tem advogado pela regulamentação das plataformas sociais. Convidado pela Unesco a participar dos debates sobre princípios globais de controle em Paris, o ministro defendeu, em entrevista à RFI Brasil, não apenas a aprovação de legislação, mas também a criação de um órgão independente para monitorar a implementação das regras.

Há ao menos três anos Barroso tem falado sobre a necessidade de criar uma legislação para enquadrar a atuação das plataformas sociais e dos usuários na internet para combater crimes e injúrias cometidas online.

À frente do processo eleitoral brasileiro de 2020, ele liderou um projeto contra a desinformação e uma parceria com as principais empresas de mídia social para reduzir os danos causados durante as eleições por discurso de ódio, desinformação e o uso de robôs.


Confira abaixo os principais trechos da entrevista, que foi editada para fins de clareza.

RFI – No seu entendimento, quais os pontos principais do ambiente digital que precisam ser legislados?

É preciso regulamentar as redes sociais e a internet, primeiro do ponto de vista econômico: para você fazer a tributação justa, para você proteger os direitos autorais, para você impedir a dominação de mercado.

Em segundo lugar, é preciso regular essas plataformas digitais do ponto de vista da proteção da privacidade. Elas têm informação sobre o seu nome, onde você mora, quanto você ganha, quem são os seus filhos. Sabem qual foi o último livro que você comprou, qual foi a última passagem e para onde você viajou. Portanto, essas informações com os dados pessoais das pessoas precisam ser protegidas para evitar o uso indevido.

Em terceiro e último lugar, é preciso regular a internet para evitar comportamentos inautênticos e, sobretudo, para impedir a veiculação de conteúdos indevidos e criminosos pela internet.

RFI – Nas eleições de 2020 e de 2022, o TSE atuou de maneira próxima às empresas digitais e pediu a retirada de páginas e o bloqueio de contas consideradas danosas ao processo eleitoral. O que a experiência brasileira mostra ao mundo?

Nós tivemos uma experiência, eu diria boa e relativamente bem-sucedida. Você precisa enfrentar dois adversários. Primeiro são os chamados comportamentos coordenados inautênticos. Isto é, a utilização de meios automatizados como bots, perfis falsos para você amplificar artificialmente a desinformação. Se você diz que a eleição foi fraudada e você atinge seus 15 seguidores, não tem um grave problema. Mas se aquilo passa a ser uma notícia veiculada para milhões de pessoas, você cria um problema institucional, sobretudo se for mentira. Portanto, a primeira coisa e a mais importante a enfrentar são os comportamentos inautênticos.

A segunda é enfrentar os conteúdos ilegítimos. Não pode ter terrorismo na rede social, não pode ter pedofilia na rede social, não pode ter uma convocação para ataque às instituições para destruir prédios ou assassinar as pessoas.

Em 2020, nós [do TSE] conseguimos fazer acordo com todas as plataformas digitais importantes da época, o que foi um avanço em relação a 2018. Tínhamos a intenção de enfrentar, primeiro, os comportamentos inautênticos. Isto é melhor, porque você não precisa ir ao conteúdo do discurso, basta verificar que está havendo uma movimentação atípica relativamente àquela postagem e, se estiver amplificando demais, você vai lá e controla. Se for uma coisa absurda ou uma coisa nociva, você tem como remover ou pelo menos diminuir o alcance.

E criamos um canal de comunicação direta com as plataformas. Monitorávamos na rede social e na imprensa o surgimento de notícias falsas. Só que nós tínhamos condições apenas de proteger a democracia e a Justiça Eleitoral contra as fake news que atacavam a democracia ou o sistema eleitoral.

Já na eleição de 2022, que era uma eleição presidencial mais complexa, começamos a ter a circulação de notícias falsas entre candidatos ou sobre candidatos. E isso é muito mais difícil de controlar, porque a distinção entre o que é tolerado e o que é intolerável nem sempre é totalmente clara. Esse talvez seja o grande problema da regulação das redes sociais: a dúvida é quem deve fazer esta identificação, este controle.

RFI – Sobre isso, o rascunho do guia de princípios da Unesco fala na possibilidade de um órgão responsável por isso. Como você vê a criação, por exemplo, de uma agência reguladora?

Não me parece bem a ideia de uma agência. O modelo que entendo é que primeiro você precisa ter uma regulação estatal, uma lei geral, com os princípios e com algumas regras mínimas. Em seguida, você precisa de um segundo nível de regulação, a autorregulação pelas plataformas – portanto, elas terem termos de uso claros sobre o que pode e o que não pode. A autorregulação e a atuação espontânea das próprias plataformas para que elas não se transformem em um instrumento destrutivo da democracia e violador dos direitos fundamentais.

E em terceiro, é preciso também o que se tem chamado de autorregulação regulada. Isto é, em vez de ter uma ampla intervenção do Estado, as próprias plataformas vão aplicar a legislação criada pelo Estado. Acho que tem que ter um controle externo, mas não feito por uma agência reguladora, mas um controle externo independente, integrado. [Esse órgão] pode ter membros do governo, das plataformas digitais, da sociedade civil e da academia para monitorar o cumprimento dessas regras.

RFI – Esse debate global leva certo tempo, e os atentados de 8 de janeiro colocaram uma urgência na pauta brasileira para tratar o assunto. O que precisa ser feito de maneira urgente?

O mundo vive um momento muito grave para a democracia e para a proteção dos direitos fundamentais, como nós assistimos nos Estados Unidos e como nós assistimos no Brasil. Boa parte do que aconteceu no 8 de janeiro foi articulado pelas redes sociais.

As mídias sociais vão ter que assumir um papel proativo na proteção da democracia e dos direitos fundamentais.
Evidentemente, se tem uma convocação para invadir um prédio público e destruir as instalações, ninguém vai discutir que isso é criminoso.

Eu acho que as plataformas precisam moderar os conteúdos em três níveis. Há um tipo de conteúdo que elas têm que tirar imediatamente do ar de ofício, ou seja, independentemente de provocação. Terrorismo, pedofilia, crimes em geral.

Em um segundo nível que eu acho que elas devem remover o conteúdo ou tomar providências mediante notificação extrajudicial, ou seja, o interessado vai lá e comunica, por exemplo, estão compartilhando fotos íntimas que eu não autorizei, e as plataformas tiram do ar.

E aí chegamos ao terceiro e mais delicado ponto. Entre esses extremos, onde há dúvida, acho que a regra é a que está no Marco Civil da Internet: você tira depois da primeira ordem judicial. Fora do crime, fora da ameaça à democracia, a maior parte das situações deve depender de uma ordem judicial.
